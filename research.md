---
layout: page
---
<div style='background: url("https://images.pexels.com/photos/1090941/pexels-photo-1090941.jpeg?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=1") center/cover no-repeat;
    padding: 60px 20px;
    text-align: center;
    color: white;
    font-size: 2.5rem;
    font-weight: bold;'>
    Research Themes
</div>


<h2> Theme 1: Open Data, Code, and Material </h2>


Reproducibility and replicability are two essential principles for scientific progress as they enhance credibility and facilitate the accumulation of knowledge. On the one hand, reproducibility ensures that findings can be independently verified using the same data and methods, while replicability enhances our confidence in findings by demonstrating that results are consistent across new datasets. One way to improve the reproducibility and replicability of scientific studies is by the adoption of Open Science practices---a set of practices aimed at making research more transparent, accessible and reproducible. 

*Our expertise*:

Members of META/e have worked/are working on the following issues relating to Open Science:

- Preregistration and Registered Reports
- The re-use of open research data
- Factors that hinder replicability of scientific studies such as publication bias and low statistical power
- Factors that hinder reproducibility of scientific studies such as lack of adherence to reporting guidelines, lack of data and code sharing and lack of adherence to best practices for data and code sharing
- Development of tool in R to help researchers prevent errors, promote best practices and extract data from published papers to conduct meta-analyses and meta-scientific projects

<br>

*Publications*:

<ul> 
<li class="publication"><strong>An Inception-Cohort Study Quantifying How Many Registered Studies Are Publicly Shared</strong> by Eline NF Ensinck, <strong>Daniël Lakens</strong>. <em>Advances in Methods and Practices in Psychological Science</em> (2025). <a href="https://doi.org/10.1177/25152459241296031" target="_blank">https://doi.org/10.1177/25152459241296031</a>.</li>
<li class="publication"><strong>The benefits of preregistration and Registered Reports</strong> by <strong>Daniël Lakens</strong>, <strong>Cristian Mesquida</strong>, <strong>Sajedeh Rasti</strong>, Massimiliano Ditroilo. <em>Evidence-Based Toxicology</em> (2024). <a href="https://doi.org/10.1080/2833373X.2024.2376046" target="_blank">https://doi.org/10.1080/2833373X.2024.2376046</a>.</li>
<li class="publication"><strong>Exploring a formal approach to selecting studies for replication: A feasibility study in social neuroscience</strong> by Peder M Isager, <strong>Daniël Lakens</strong>, Thed van Leeuwen, Anna E van't Veer. <em>Cortex</em> (2024). <a href="https://doi.org/10.1016/j.cortex.2023.10.012" target="_blank">https://doi.org/10.1016/j.cortex.2023.10.012</a>.</li>
<li class="publication"><strong>When and How to Deviate from a Preregistration</strong> by <strong>Daniël Lakens</strong>. <em>Collabra: Psychology</em> (2024). <a href="https://doi.org/10.1525/collabra.117094" target="_blank">https://doi.org/10.1525/collabra.117094</a>.</li>
<li class="publication"><strong>Is my study useless? Why researchers need methodological review boards.</strong> by <strong>Daniël Lakens</strong>. <em>Nature</em> (2023). <a href="https://doi.org/10.1038/d41586-022-04504-8" target="_blank">https://doi.org/10.1038/d41586-022-04504-8</a>.</li>
<li class="publication"><strong>Publication bias, statistical power and reporting practices in the Journal of Sports Sciences: potential barriers to replicability</strong> by <strong>Cristian Mesquida</strong>, Jennifer Murphy, <strong>Daniël Lakens</strong>, Joe Warne. <em>Journal of sports sciences</em> (2023). <a href="https://doi.org/10.1080/02640414.2023.2269357" target="_blank">https://doi.org/10.1080/02640414.2023.2269357</a>.</li>


</ul>

<br>

<h2> Theme 2: Team Science </h2>

As scientific and societal challenges grow increasingly complex, tackling them requires research collaborations that span disciplines, organizations, and geographic regions. The field of Science of Team Science (SciTS) emerged to empirically explore questions from funding agencies, administrators, and researchers about the benefits of team science, as well as effective strategies for leading, participating in, facilitating, and supporting scientific teams.

*Our expertise*:

Members of META/e have worked on/are working on the following issues relating to Team Science:

- Funding agencies’ procedures for evaluating inter-disciplinary and trans-disciplinary grant proposals
- The nature, benefits and promotion of scientific coordination
- Diversity and inclusion
- Impact of competition on information sharing and idea selection in science

<br>

*Publications*:

<ul>
<li class="publication"><strong>A Framework for Describing the Levels of Scientific Coordination</strong> by <strong>Sajedeh Rasti</strong>, <strong>Krist Vaesen</strong>, <strong>Daniël Lakens</strong>. <em>OSF</em> (2025). <a href="https://osf.io/preprints/psyarxiv/eq269_v1" target="_blank">https://osf.io/preprints/psyarxiv/eq269_v1</a>.</li>
<li class="publication"><strong>The Need for Scientific Coordination</strong> by <strong>Sajedeh Rasti</strong>, <strong>Krist Vaesen</strong>, <strong>Daniël Lakens</strong>. <em>OSF</em> (2025). <a href="https://osf.io/preprints/psyarxiv/vjcfk_v2" target="_blank">https://osf.io/preprints/psyarxiv/vjcfk_v2</a>.</li>
<li class="publication"><strong>How qualitative criteria can improve the assessment process of interdisciplinary research proposals</strong> by Anne-Floor Schölvinck, Duygu Uygun-Tunç, <strong>Daniël Lakens</strong>, <strong>Krist Vaesen</strong>,  Laurens K Hessels. <em>Research Evaluation</em> (2024). <a href="https://doi.org/10.1093/reseval/rvae049" target="_blank">https://doi.org/10.1093/reseval/rvae049</a>.</li>
<li class="publication"><strong>Shifting the level of selection in science</strong> by Leo Tiokhin, Karthik Panchanathan, Paul E Smaldino, <strong>Daniël Lakens</strong>. <em>Perspectives on Psychological Science</em> (2024). <a href="https://doi.org/10.1177/17456916231182568" target="_blank">https://doi.org/10.1177/17456916231182568</a>.
</li>
<li class="publication"><strong>Epistemic Inclusion as the Key to Benefiting from Cognitive Diversity in Science</strong> by <strong>Vlasta Sikimić</strong>. <em>Social Epistemology</em> (2023). <a href="https://doi.org/10.1080/02691728.2023.2258831" target="_blank">https://doi.org/10.1080/02691728.2023.2258831</a>.</li>


</ul>

<br>

<h2> Theme 3: AI & Science </h2>

Researchers are increasingly using AI tools for their studies, assistance in writing and even for feedback and evaluation of their work. AI may help in optimizing scientific teams and automating checks on research methodology. AI may also assess the quality of individual papers and may provide insights for science funding agencies to make better decisions. Yet, the use of AI in metascience needs to be responsible. META/e works on creating software for methodology checking, promoting the responsible use of AI in science funding decisions and creating educational material on critical thinking about AI-generated content.

*Our expertise*:

Members of META/e have worked on/are working on the following issues relating to AI & Science:

- Automated grant review 
- Use of AI for detecting optimal team composition in science 
- Automated checkers of research quality 
- Developing critical thinking skills in reasoning about AI generated content 

<br>

*Publications*:

<ul>
<li class="publication"><strong>Global justice and the use of AI in education: ethical and epistemic aspects</strong> by Aleksandra Vučković, <strong>Vlasta Sikimić</strong>. <em>AI & Society</em> (2024). <a href="https://doi.org/10.1007/s13194-022-00478-6" target="_blank">https://doi.org/10.1007/s00146-024-02076-x</a>.</li>
<li class="publication"><strong>Machine learning in scientific grant review: algorithmically predicting project efficiency in high energy physics</strong> by <strong>Vlasta Sikimić</strong>, Sandro Radovanović. <em>European Journal for Philosophy of Science</em> (2022). <a href="https://doi.org/10.1007/s13194-022-00478-6" target="_blank">https://doi.org/10.1007/s13194-022-00478-6</a>.</li>

</ul>

<br>

<h2> Theme 4: Assorted issues </h2>

*Our expertise*:

Members of META/e have worked on/are working on the following assorted meta-scientific topics:

- PhD attrition
- Researchers’ academic values
- Science funding
- Research integrity
- Meaningfully interpreting experimental results
- Costs of restricted access to publicly funded science

<br>

*Publications*:

<ul>
<li class="publication"><strong>Academic research values: Conceptualization and initial steps of scale development</strong> by <strong>Andrea Kis</strong>, Elena Mas Tur, <strong>Krist Vaesen</strong>, <strong>Wybo Houkes</strong>, <strong>Daniël Lakens</strong>. <em>Plos one</em> (2025). <a href="https://doi.org/10.1371/journal.pone.0318086" target="_blank">https://doi.org/10.1371/journal.pone.0318086</a>.</li>
<li class="publication"><strong>Questionable research practices in competitive grant funding: A survey</strong> by Stijn Conix, Steven De Peuter, Andreas De Block, <strong>Krist Vaesen</strong>. <em>Plos one</em> (2023). <a href="https://doi.org/10.1371/journal.pone.0293310" target="_blank">https://doi.org/10.1371/journal.pone.0293310</a>.</li>
<li class="publication"><strong>Leaving academia: PhD attrition and unhealthy research environments</strong> by <strong>Andrea Kis</strong>, Elena Mas Tur, <strong>Daniël Lakens</strong>, <strong>Krist Vaesen</strong>, <strong>Wybo Houkes</strong>. <em>Plos one</em> (2022). <a href="https://doi.org/10.1371/journal.pone.0274976" target="_blank">https://doi.org/10.1371/journal.pone.0274976</a>.</li>
<li class="publication"><strong>How to Improve Research Funding in Academia? Lessons From the COVID-19 Crisis</strong> by <strong>Vlasta Sikimić</strong>. <em>Frontiers in Research Metrics and Analytics</em> (2022). <a href="https://doi.org/10.3389/frma.2022.777781" target="_blank">https://doi.org/10.3389/frma.2022.777781</a>.</li>
<li class="publication"><strong>Grant writing and grant peer review as questionable research practices</strong> by Stijn Conix, Andreas De Block, <strong>Krist Vaesen</strong>. <em>F1000Research</em> (2021). <a href="https://doi.org/10.12688/f1000research.73893.2" target="_blank">https://doi.org/10.12688/f1000research.73893.2</a>.</li>

</ul>




